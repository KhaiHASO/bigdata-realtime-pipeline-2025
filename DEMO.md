# Demo Scenarios - Real-Time Streaming Pipeline

T√†i li·ªáu n√†y m√¥ t·∫£ c√°c k·ªãch b·∫£n demo ƒë·ªÉ test v√† verify to√†n b·ªô pipeline: **Kafka ‚Üí Flink ‚Üí Cassandra**

## M·ª•c l·ª•c

1. [Demo 1: Pipeline C∆° B·∫£n](#demo-1-pipeline-c∆°-b·∫£n)
2. [Demo 2: Real-Time Streaming](#demo-2-real-time-streaming)
3. [Demo 3: Airflow Orchestration](#demo-3-airflow-orchestration)
4. [Demo 4: X·ª≠ L√Ω L·ªói v√† Recovery](#demo-4-x·ª≠-l√Ω-l·ªói-v√†-recovery)
5. [Demo 5: Monitoring v√† Verification](#demo-5-monitoring-v√†-verification)
6. [Demo 6: Query v√† Analysis](#demo-6-query-v√†-analysis)
7. [Demo 7: End-to-End Workflow](#demo-7-end-to-end-workflow)

---

## Demo 1: Pipeline C∆° B·∫£n

**M·ª•c ti√™u:** Test pipeline c∆° b·∫£n t·ª´ Producer ‚Üí Kafka ‚Üí Flink ‚Üí Cassandra

### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông h·ªá th·ªëng

```bash
# D·ª´ng v√† x√≥a volumes c≈© (n·∫øu c√≥)
docker compose down -v

# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services
docker compose up -d --build

# ƒê·ª£i services kh·ªüi ƒë·ªông (60-90 gi√¢y)
docker compose ps
```

### B∆∞·ªõc 2: Kh·ªüi t·∫°o Cassandra

```bash
# ƒê·ª£i Cassandra s·∫µn s√†ng
sleep 30

# Kh·ªüi t·∫°o keyspace v√† table
docker exec cassandra cqlsh -f /init.cql

# Verify
docker exec cassandra cqlsh -e "DESCRIBE KEYSPACE realtime;"
```

### B∆∞·ªõc 3: T·∫°o Kafka Topic

```bash
# T·∫°o topic "users"
docker exec kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic users \
  --partitions 1 \
  --replication-factor 1

# Verify topic
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list
```

### B∆∞·ªõc 4: Ch·∫°y Producer

```bash
# C√†i ƒë·∫∑t dependencies
cd producer
pip install -r requirements.txt

# Ch·∫°y producer (g·ª≠i 10 messages)
python producer.py
```

**Ho·∫∑c ch·ªânh s·ª≠a s·ªë l∆∞·ª£ng messages trong producer.py:**
```python
send_messages(producer, TOPIC_NAME, num_messages=10, delay=1)
```

### B∆∞·ªõc 5: Submit Flink Job

```bash
# Submit Flink job
docker exec -it flink-jobmanager /opt/flink/bin/flink run \
  -py /opt/flink/usrlib/job.py
```

**Ho·∫∑c check Flink UI:** http://localhost:8081

### B∆∞·ªõc 6: Verify Data trong Cassandra

```bash
# ƒê·∫øm s·ªë records
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"

# Xem 5 records ƒë·∫ßu ti√™n
docker exec cassandra cqlsh -e "SELECT * FROM realtime.users LIMIT 5;"
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ Producer g·ª≠i th√†nh c√¥ng messages v√†o Kafka
- ‚úÖ Flink job ƒë·ªçc t·ª´ Kafka v√† ghi v√†o Cassandra
- ‚úÖ Data xu·∫•t hi·ªán trong Cassandra table

---

## Demo 2: Real-Time Streaming

**M·ª•c ti√™u:** Test real-time streaming v·ªõi continuous data flow

### B∆∞·ªõc 1: ƒê·∫£m b·∫£o Flink Job ƒëang ch·∫°y

```bash
# Check Flink UI: http://localhost:8081
# Ho·∫∑c check logs
docker compose logs flink-taskmanager | tail -20
```

### B∆∞·ªõc 2: Ch·∫°y Producer li√™n t·ª•c

M·ªü terminal m·ªõi v√† ch·∫°y producer v·ªõi delay ng·∫Øn:

```bash
cd producer
python -c "
from producer import *
import time
producer = create_producer()
for i in range(50):
    send_messages(producer, 'users', num_messages=1, delay=0.5)
    print(f'Sent batch {i+1}')
    time.sleep(2)
producer.close()
"
```

### B∆∞·ªõc 3: Monitor Real-Time

**Terminal 1: Monitor Kafka messages**
```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic users \
  --from-beginning
```

**Terminal 2: Monitor Cassandra (real-time count)**
```bash
watch -n 2 "docker exec cassandra cqlsh -e 'SELECT COUNT(*) FROM realtime.users;'"
```

**Terminal 3: Monitor Flink logs**
```bash
docker compose logs -f flink-taskmanager
```

### B∆∞·ªõc 4: Verify Real-Time Processing

```bash
# Check s·ªë l∆∞·ª£ng records tƒÉng d·∫ßn
for i in {1..10}; do
  echo "Count at $(date):"
  docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"
  sleep 5
done
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ Messages ƒë∆∞·ª£c x·ª≠ l√Ω real-time
- ‚úÖ Data xu·∫•t hi·ªán trong Cassandra ngay sau khi producer g·ª≠i
- ‚úÖ Kh√¥ng c√≥ lag ƒë√°ng k·ªÉ gi·ªØa Kafka v√† Cassandra

---

## Demo 3: Airflow Orchestration

**M·ª•c ti√™u:** Test Airflow DAG ƒë·ªÉ orchestrate pipeline

### B∆∞·ªõc 1: Truy c·∫≠p Airflow UI

```bash
# M·ªü browser: http://localhost:8080
# Login: admin / admin
```

### B∆∞·ªõc 2: Enable DAG

1. V√†o Airflow UI
2. T√¨m DAG: `kafka_flink_pipeline`
3. Toggle ON ƒë·ªÉ enable DAG

### B∆∞·ªõc 3: Trigger DAG manually

1. Click v√†o DAG `kafka_flink_pipeline`
2. Click n√∫t "Play" (‚ñ∂Ô∏è) ƒë·ªÉ trigger
3. Ch·ªçn "Trigger DAG"

### B∆∞·ªõc 4: Monitor DAG Execution

1. Click v√†o DAG ƒë·ªÉ xem Graph View
2. Xem c√°c tasks:
   - `wait_for_services`
   - `run_producer`
   - `submit_flink_job`
   - `verify_cassandra`

### B∆∞·ªõc 5: Check Logs

```bash
# Xem Airflow logs
docker compose logs -f airflow-scheduler

# Xem DAG logs trong Airflow UI
# Click v√†o task ‚Üí Logs
```

### B∆∞·ªõc 6: Verify Results

```bash
# Check data trong Cassandra
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ DAG ch·∫°y th√†nh c√¥ng
- ‚úÖ T·∫•t c·∫£ tasks completed
- ‚úÖ Data ƒë∆∞·ª£c t·∫°o v√† verify

---

## Demo 4: X·ª≠ L√Ω L·ªói v√† Recovery

**M·ª•c ti√™u:** Test kh·∫£ nƒÉng x·ª≠ l√Ω l·ªói v√† recovery c·ªßa pipeline

### Scenario 1: Flink Job Crash

**B∆∞·ªõc 1: Stop Flink TaskManager**
```bash
docker compose stop flink-taskmanager
```

**B∆∞·ªõc 2: G·ª≠i messages v√†o Kafka**
```bash
cd producer
python producer.py  # G·ª≠i messages
```

**B∆∞·ªõc 3: Restart Flink**
```bash
docker compose start flink-taskmanager
```

**B∆∞·ªõc 4: Verify Recovery**
```bash
# Check Flink job restart
docker compose logs flink-taskmanager | tail -20

# Verify data ƒë∆∞·ª£c x·ª≠ l√Ω (Flink ƒë·ªçc t·ª´ earliest offset)
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"
```

### Scenario 2: Kafka Restart

**B∆∞·ªõc 1: Stop Kafka**
```bash
docker compose stop kafka
```

**B∆∞·ªõc 2: Producer s·∫Ω fail**
```bash
cd producer
python producer.py  # S·∫Ω b√°o l·ªói connection
```

**B∆∞·ªõc 3: Restart Kafka**
```bash
docker compose start kafka
sleep 10
```

**B∆∞·ªõc 4: Producer retry**
```bash
python producer.py  # S·∫Ω th√†nh c√¥ng
```

### Scenario 3: Cassandra Restart

**B∆∞·ªõc 1: Stop Cassandra**
```bash
docker compose stop cassandra
```

**B∆∞·ªõc 2: Flink s·∫Ω fail khi ghi**
```bash
# Check Flink logs
docker compose logs flink-taskmanager | grep -i error
```

**B∆∞·ªõc 3: Restart Cassandra**
```bash
docker compose start cassandra
sleep 30
```

**B∆∞·ªõc 4: Verify Recovery**
```bash
# Flink s·∫Ω t·ª± ƒë·ªông retry v√† ghi th√†nh c√¥ng
docker compose logs flink-taskmanager | tail -20
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ Pipeline c√≥ kh·∫£ nƒÉng recovery sau khi service restart
- ‚úÖ Kh√¥ng m·∫•t data (Kafka l∆∞u messages)
- ‚úÖ Flink retry v√† ti·∫øp t·ª•c x·ª≠ l√Ω

---

## Demo 5: Monitoring v√† Verification

**M·ª•c ti√™u:** Monitor to√†n b·ªô pipeline v√† verify data integrity

### B∆∞·ªõc 1: Monitor T·∫•t C·∫£ Services

**Terminal 1: Kafka Metrics**
```bash
# List topics v√† partitions
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic users

# Consumer lag (n·∫øu c√≥ consumer group)
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group flink-users-consumer --describe
```

**Terminal 2: Flink Metrics**
```bash
# Flink UI: http://localhost:8081
# Check:
# - Running Jobs
# - Task Managers
# - Job Metrics (throughput, latency)
```

**Terminal 3: Cassandra Metrics**
```bash
# Node status
docker exec cassandra nodetool status

# Table statistics
docker exec cassandra nodetool tablestats realtime.users
```

**Terminal 4: Airflow Metrics**
```bash
# Airflow UI: http://localhost:8080
# Check DAG runs, task durations, success rates
```

### B∆∞·ªõc 2: Data Integrity Check

**B∆∞·ªõc 2.1: Count Messages trong Kafka**
```bash
# ƒê·∫øm messages trong Kafka topic
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic users \
  --time -1
```

**B∆∞·ªõc 2.2: Count Records trong Cassandra**
```bash
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"
```

**B∆∞·ªõc 2.3: So s√°nh**
```bash
# Messages trong Kafka ‚âà Records trong Cassandra
# (C√≥ th·ªÉ c√≥ s·ª± kh√°c bi·ªát nh·ªè do timing)
```

### B∆∞·ªõc 3: Sample Data Verification

```bash
# L·∫•y sample t·ª´ Kafka
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic users \
  --from-beginning \
  --max-messages 5

# L·∫•y sample t·ª´ Cassandra
docker exec cassandra cqlsh -e "SELECT id, name, email, ts FROM realtime.users LIMIT 5;"

# So s√°nh: Data ph·∫£i match
```

### B∆∞·ªõc 4: Performance Metrics

```bash
# Flink throughput
# Check trong Flink UI: Jobs ‚Üí Metrics ‚Üí Records per second

# Latency
# Check trong Flink UI: Jobs ‚Üí Metrics ‚Üí Latency

# Cassandra write performance
docker exec cassandra nodetool tablestats realtime.users | grep -i "write"
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ T·∫•t c·∫£ services healthy
- ‚úÖ Data integrity ƒë∆∞·ª£c maintain
- ‚úÖ Performance metrics trong acceptable range

---

## Demo 6: Query v√† Analysis

**M·ª•c ti√™u:** Query v√† analyze data trong Cassandra

### B∆∞·ªõc 1: Basic Queries

```bash
# Connect to Cassandra
docker exec -it cassandra cqlsh

# Use keyspace
USE realtime;

# Count total records
SELECT COUNT(*) FROM users;

# View all columns
SELECT * FROM users LIMIT 10;
```

### B∆∞·ªõc 2: Query by Email (s·ª≠ d·ª•ng index)

```bash
# T√¨m user by email
SELECT * FROM users WHERE email = 'alice.smith@example.com';

# List all unique emails
SELECT DISTINCT email FROM users LIMIT 20;
```

### B∆∞·ªõc 3: Time-based Queries

```bash
# Users created in last hour
SELECT * FROM users WHERE ts > dateOf(now()) - 1h ALLOW FILTERING;

# Count by time range
SELECT COUNT(*) FROM users WHERE ts > '2024-01-01' ALLOW FILTERING;
```

### B∆∞·ªõc 4: Data Analysis

```bash
# Top 10 most recent users
SELECT id, name, email, ts FROM users LIMIT 10;

# Count by domain
SELECT email, COUNT(*) FROM users GROUP BY email ALLOW FILTERING;
```

### B∆∞·ªõc 5: Export Data

```bash
# Export to CSV
docker exec cassandra cqlsh -e "
COPY realtime.users (id, name, email, ts) TO '/tmp/users.csv' WITH HEADER = true;
"

# Copy file out
docker cp cassandra:/tmp/users.csv ./users_export.csv

# View
head -10 users_export.csv
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ C√≥ th·ªÉ query data d·ªÖ d√†ng
- ‚úÖ Index ho·∫°t ƒë·ªông t·ªët
- ‚úÖ C√≥ th·ªÉ export data ƒë·ªÉ analysis

---

## Demo 7: End-to-End Workflow

**M·ª•c ti√™u:** Ch·∫°y to√†n b·ªô workflow t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi

### Scenario: Complete Pipeline Test

**B∆∞·ªõc 1: Clean Start**
```bash
# Stop v√† x√≥a t·∫•t c·∫£
docker compose down -v

# Kh·ªüi ƒë·ªông l·∫°i
docker compose up -d --build

# ƒê·ª£i services ready
sleep 90
```

**B∆∞·ªõc 2: Initialize**
```bash
# Initialize Cassandra
docker exec cassandra cqlsh -f /init.cql

# Create Kafka topic
docker exec kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic users \
  --partitions 1 \
  --replication-factor 1
```

**B∆∞·ªõc 3: Start Flink Job**
```bash
# Submit Flink job
docker exec -d flink-jobmanager /opt/flink/bin/flink run \
  -py /opt/flink/usrlib/job.py

# Verify job running
sleep 10
curl http://localhost:8081/jobs
```

**B∆∞·ªõc 4: Generate Data**
```bash
# Run producer v·ªõi nhi·ªÅu messages
cd producer
python -c "
from producer import *
producer = create_producer()
send_messages(producer, 'users', num_messages=100, delay=0.1)
producer.close()
"
```

**B∆∞·ªõc 5: Monitor Progress**
```bash
# Watch data flow
watch -n 2 "docker exec cassandra cqlsh -e 'SELECT COUNT(*) FROM realtime.users;'"
```

**B∆∞·ªõc 6: Verify Results**
```bash
# Final count
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"

# Sample data
docker exec cassandra cqlsh -e "SELECT * FROM realtime.users LIMIT 10;"

# Check Flink job status
curl http://localhost:8081/jobs
```

**B∆∞·ªõc 7: Airflow Orchestration**
```bash
# Trigger Airflow DAG
# 1. Go to http://localhost:8080
# 2. Enable DAG: kafka_flink_pipeline
# 3. Trigger DAG
# 4. Monitor execution
```

**B∆∞·ªõc 8: Final Verification**
```bash
# Check all services healthy
docker compose ps

# Check data integrity
KAFKA_COUNT=$(docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic users --time -1 | awk -F: '{sum+=$3} END {print sum}')

CASSANDRA_COUNT=$(docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;" | grep -o '[0-9]*' | head -1)

echo "Kafka messages: $KAFKA_COUNT"
echo "Cassandra records: $CASSANDRA_COUNT"
echo "Difference: $((KAFKA_COUNT - CASSANDRA_COUNT))"
```

### K·∫øt qu·∫£ mong ƒë·ª£i:
- ‚úÖ To√†n b·ªô pipeline ho·∫°t ƒë·ªông end-to-end
- ‚úÖ Data ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u tr·ªØ ƒë√∫ng
- ‚úÖ T·∫•t c·∫£ services healthy
- ‚úÖ Airflow orchestration th√†nh c√¥ng

---

## Quick Reference Commands

### Start/Stop Services
```bash
# Start all
docker compose up -d

# Stop all
docker compose down

# Restart specific service
docker compose restart kafka
```

### Check Service Status
```bash
# All services
docker compose ps

# Specific service logs
docker compose logs -f kafka
docker compose logs -f flink-jobmanager
docker compose logs -f cassandra
docker compose logs -f airflow-webserver
```

### Kafka Commands
```bash
# List topics
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Describe topic
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --describe --topic users

# Consume messages
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic users --from-beginning
```

### Flink Commands
```bash
# List jobs
curl http://localhost:8081/jobs

# Submit job
docker exec flink-jobmanager /opt/flink/bin/flink run -py /opt/flink/usrlib/job.py

# Cancel job
docker exec flink-jobmanager /opt/flink/bin/flink cancel <job-id>
```

### Cassandra Commands
```bash
# Connect
docker exec -it cassandra cqlsh

# Count records
docker exec cassandra cqlsh -e "SELECT COUNT(*) FROM realtime.users;"

# Query data
docker exec cassandra cqlsh -e "SELECT * FROM realtime.users LIMIT 10;"
```

### Airflow Commands
```bash
# Check DAGs
docker exec airflow-webserver airflow dags list

# Trigger DAG
docker exec airflow-webserver airflow dags trigger kafka_flink_pipeline

# List tasks
docker exec airflow-webserver airflow tasks list kafka_flink_pipeline
```

---

## Troubleshooting Tips

### Flink Job kh√¥ng ch·∫°y
```bash
# Check Flink logs
docker compose logs flink-taskmanager

# Check Kafka connection
docker exec flink-taskmanager ping kafka

# Check Cassandra connection
docker exec flink-taskmanager ping cassandra
```

### Data kh√¥ng xu·∫•t hi·ªán trong Cassandra
```bash
# Check Flink job status
curl http://localhost:8081/jobs

# Check Kafka c√≥ messages kh√¥ng
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic users --from-beginning --max-messages 5

# Check Flink logs for errors
docker compose logs flink-taskmanager | grep -i error
```

### Airflow DAG kh√¥ng ch·∫°y
```bash
# Check Airflow logs
docker compose logs airflow-scheduler

# Check MySQL connection
docker exec airflow-webserver env | grep SQL_ALCHEMY_CONN

# Restart Airflow
docker compose restart airflow-webserver airflow-scheduler
```

---

## Performance Benchmarks

### Expected Performance

- **Kafka Producer:** ~1000 messages/second
- **Flink Processing:** ~500-1000 records/second
- **Cassandra Writes:** ~500-1000 writes/second
- **End-to-End Latency:** < 5 seconds

### Load Testing

```bash
# High volume test
cd producer
python -c "
from producer import *
import time
producer = create_producer()
start = time.time()
send_messages(producer, 'users', num_messages=1000, delay=0)
end = time.time()
print(f'Produced 1000 messages in {end-start:.2f} seconds')
print(f'Throughput: {1000/(end-start):.2f} messages/sec')
producer.close()
"
```

---

## Next Steps

Sau khi ho√†n th√†nh c√°c demos, b·∫°n c√≥ th·ªÉ:

1. **T√πy ch·ªânh Producer:** Th√™m fields, change data format
2. **Enhance Flink Job:** Th√™m transformations, filtering, aggregations
3. **Optimize Cassandra:** Tuning table structure, indexes
4. **Scale Services:** Th√™m more Flink TaskManagers, Kafka partitions
5. **Add Monitoring:** Prometheus, Grafana dashboards
6. **Production Setup:** Security, authentication, backup strategies

---

**Happy Testing! üöÄ**

