services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - pipeline-network

  kafka:
    image: confluentinc/cp-server:7.4.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    networks:
      - pipeline-network

  cassandra:
    image: cassandra:4.1
    hostname: cassandra
    container_name: cassandra
    ports:
      - "9042:9042"
      - "7000:7000"
      - "7001:7001"
    environment:
      CASSANDRA_CLUSTER_NAME: "realtime-cluster"
      CASSANDRA_DC: "datacenter1"
      CASSANDRA_RACK: "rack1"
    volumes:
      - cassandra-data:/var/lib/cassandra
      - ./cassandra/init.cql:/init.cql
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD-SHELL", "nodetool status | grep -E '^UN' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  flink-jobmanager:
    build:
      context: ./flink
      dockerfile: Dockerfile
    hostname: flink-jobmanager
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 1
    volumes:
      - ./flink:/opt/flink/usrlib
    networks:
      - pipeline-network

  flink-taskmanager:
    build:
      context: ./flink
      dockerfile: Dockerfile
    hostname: flink-taskmanager
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    scale: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 1
    volumes:
      - ./flink:/opt/flink/usrlib
    networks:
      - pipeline-network

  mysql:
    image: mysql:8.0
    hostname: mysql
    container_name: mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: airflow
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    volumes:
      - mysql-data:/var/lib/mysql
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-pairflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    hostname: airflow-webserver
    container_name: airflow-webserver
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_started
      cassandra:
        condition: service_started
      flink-jobmanager:
        condition: service_started
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqldb://airflow:airflow@mysql:3306/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    networks:
      - pipeline-network
    command: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin 2>/dev/null || true &&
        airflow webserver
      "
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    hostname: airflow-scheduler
    container_name: airflow-scheduler
    depends_on:
      mysql:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqldb://airflow:airflow@mysql:3306/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    networks:
      - pipeline-network
    command: scheduler

networks:
  pipeline-network:
    driver: bridge

volumes:
  cassandra-data:
  airflow-logs:
  mysql-data:

